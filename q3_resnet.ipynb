{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3: Even deeper! Resnet18 for PASCAL classification (15 pts)\n",
    "\n",
    "Hopefully we all got much better accuracy with the deeper model! Since 2012, much deeper architectures have been proposed. [ResNet](https://arxiv.org/abs/1512.03385) is one of the popular ones. In this task, we attempt to further improve the performance with the “very deep” ResNet-18 architecture.\n",
    "\n",
    "\n",
    "## 3.1 Build ResNet-18 (1 pts)\n",
    "Write a network modules for the Resnet-18 architecture (refer to the original paper). You can use `torchvision.models` for this section, so it should be very easy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.0066, -0.0033, -0.0072,  0.0302,  0.0039,  0.0047, -0.0424, -0.0206,\n",
      "        -0.0154,  0.0231,  0.0344, -0.0125,  0.0010, -0.0060, -0.0230, -0.0085,\n",
      "         0.0352, -0.0417,  0.0276,  0.0302], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import trainer\n",
    "from utils import ARGS\n",
    "from simple_cnn import SimpleCNN\n",
    "from voc_dataset import VOCDataset\n",
    "\n",
    "\n",
    "# you could write the whole class....\n",
    "# or one line :D\n",
    "ResNet = models.resnet18()\n",
    "ResNet.fc = nn.Linear(512,20,bias=True)\n",
    "print(ResNet.fc.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params = ResNet.named_parameters()\n",
    "with torch.no_grad():\n",
    "    grad_dict = dict()\n",
    "    for i, named_param in enumerate(params):\n",
    "        name, param = named_param\n",
    "        if 'conv' in name:\n",
    "            layer_name = name.split('.')[0]\n",
    "            if layer_name not in grad_dict:\n",
    "                grad_dict[layer_name] = []\n",
    "            grad_dict[layer_name].append(param.flatten())\n",
    "            cout, cin, size, size = param.shape\n",
    "        #print(name)\n",
    "\n",
    "    for key, grad_list in grad_dict.items():\n",
    "        grad_list = torch.cat(grad_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Add Tensorboard Summaries (6 pts)\n",
    "You should've already written tensorboard summary generation code into `trainer.py` from q1. However, you probably just added the most basic summary features. Please implement the more advanced summaries listed here:\n",
    "* training loss (should be done)\n",
    "* testing MAP curves (should be done)\n",
    "* learning rate\n",
    "* histogram of gradients\n",
    "\n",
    "## 3.3 Train and Test (8 pts)\n",
    "Use the same hyperparameter settings from Task 2, and train the model for 50 epochs. Report tensorboard screenshots for *all* of the summaries listed above (for image summaries show screenshots at $n \\geq 3$ iterations)\n",
    "\n",
    "**REMEMBER TO SAVE A MODEL AT THE END OF TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0 (0%)]\tLoss: 0.774568\n",
      "Train Epoch: 0 [100 (64%)]\tLoss: 0.217245\n",
      "Train Epoch: 1 [200 (27%)]\tLoss: 0.210178\n",
      "Train Epoch: 1 [300 (91%)]\tLoss: 0.248149\n",
      "Train Epoch: 2 [400 (55%)]\tLoss: 0.199472\n",
      "Train Epoch: 3 [500 (18%)]\tLoss: 0.225377\n",
      "Train Epoch: 3 [600 (82%)]\tLoss: 0.172484\n",
      "Train Epoch: 4 [700 (46%)]\tLoss: 0.212226\n",
      "Train Epoch: 5 [800 (10%)]\tLoss: 0.191931\n",
      "Train Epoch: 5 [900 (73%)]\tLoss: 0.221456\n",
      "Train Epoch: 6 [1000 (37%)]\tLoss: 0.191145\n",
      "Train Epoch: 7 [1100 (1%)]\tLoss: 0.184086\n",
      "Train Epoch: 7 [1200 (64%)]\tLoss: 0.197617\n",
      "Train Epoch: 8 [1300 (28%)]\tLoss: 0.208478\n",
      "Train Epoch: 8 [1400 (92%)]\tLoss: 0.161722\n",
      "Train Epoch: 9 [1500 (55%)]\tLoss: 0.183152\n",
      "Train Epoch: 10 [1600 (19%)]\tLoss: 0.140961\n",
      "Train Epoch: 10 [1700 (83%)]\tLoss: 0.163738\n",
      "Train Epoch: 11 [1800 (46%)]\tLoss: 0.133792\n",
      "Train Epoch: 12 [1900 (10%)]\tLoss: 0.168048\n",
      "Train Epoch: 12 [2000 (74%)]\tLoss: 0.168653\n",
      "Train Epoch: 13 [2100 (38%)]\tLoss: 0.138353\n",
      "Train Epoch: 14 [2200 (1%)]\tLoss: 0.148634\n",
      "Train Epoch: 14 [2300 (65%)]\tLoss: 0.132664\n",
      "Train Epoch: 15 [2400 (29%)]\tLoss: 0.139118\n",
      "Train Epoch: 15 [2500 (92%)]\tLoss: 0.122545\n",
      "Train Epoch: 16 [2600 (56%)]\tLoss: 0.139761\n",
      "Train Epoch: 17 [2700 (20%)]\tLoss: 0.097369\n",
      "Train Epoch: 17 [2800 (83%)]\tLoss: 0.155642\n",
      "Train Epoch: 18 [2900 (47%)]\tLoss: 0.112209\n",
      "Train Epoch: 19 [3000 (11%)]\tLoss: 0.092895\n",
      "Train Epoch: 19 [3100 (75%)]\tLoss: 0.076714\n",
      "Train Epoch: 20 [3200 (38%)]\tLoss: 0.083424\n",
      "Train Epoch: 21 [3300 (2%)]\tLoss: 0.074741\n",
      "Train Epoch: 21 [3400 (66%)]\tLoss: 0.074861\n",
      "Train Epoch: 22 [3500 (29%)]\tLoss: 0.062936\n",
      "Train Epoch: 22 [3600 (93%)]\tLoss: 0.062672\n",
      "Train Epoch: 23 [3700 (57%)]\tLoss: 0.057605\n",
      "Train Epoch: 24 [3800 (20%)]\tLoss: 0.051928\n",
      "Train Epoch: 24 [3900 (84%)]\tLoss: 0.049192\n",
      "Train Epoch: 25 [4000 (48%)]\tLoss: 0.050109\n",
      "Train Epoch: 26 [4100 (11%)]\tLoss: 0.039428\n",
      "Train Epoch: 26 [4200 (75%)]\tLoss: 0.026693\n",
      "Train Epoch: 27 [4300 (39%)]\tLoss: 0.043190\n",
      "Train Epoch: 28 [4400 (3%)]\tLoss: 0.020502\n",
      "Train Epoch: 28 [4500 (66%)]\tLoss: 0.022479\n",
      "Train Epoch: 29 [4600 (30%)]\tLoss: 0.015759\n",
      "Train Epoch: 29 [4700 (94%)]\tLoss: 0.024647\n",
      "Train Epoch: 30 [4800 (57%)]\tLoss: 0.014559\n",
      "Train Epoch: 31 [4900 (21%)]\tLoss: 0.011714\n",
      "Train Epoch: 31 [5000 (85%)]\tLoss: 0.014209\n",
      "Train Epoch: 32 [5100 (48%)]\tLoss: 0.010125\n",
      "Train Epoch: 33 [5200 (12%)]\tLoss: 0.007443\n",
      "Train Epoch: 33 [5300 (76%)]\tLoss: 0.005764\n",
      "Train Epoch: 34 [5400 (39%)]\tLoss: 0.004252\n",
      "Train Epoch: 35 [5500 (3%)]\tLoss: 0.006565\n",
      "Train Epoch: 35 [5600 (67%)]\tLoss: 0.003613\n",
      "Train Epoch: 36 [5700 (31%)]\tLoss: 0.007355\n",
      "Train Epoch: 36 [5800 (94%)]\tLoss: 0.005959\n",
      "Train Epoch: 37 [5900 (58%)]\tLoss: 0.006650\n",
      "Train Epoch: 38 [6000 (22%)]\tLoss: 0.001894\n",
      "Train Epoch: 38 [6100 (85%)]\tLoss: 0.003414\n",
      "Train Epoch: 39 [6200 (49%)]\tLoss: 0.003833\n",
      "Train Epoch: 40 [6300 (13%)]\tLoss: 0.002110\n",
      "Train Epoch: 40 [6400 (76%)]\tLoss: 0.002179\n",
      "Train Epoch: 41 [6500 (40%)]\tLoss: 0.003905\n",
      "Train Epoch: 42 [6600 (4%)]\tLoss: 0.004638\n",
      "Train Epoch: 42 [6700 (68%)]\tLoss: 0.002099\n",
      "Train Epoch: 43 [6800 (31%)]\tLoss: 0.001898\n",
      "Train Epoch: 43 [6900 (95%)]\tLoss: 0.003052\n",
      "Train Epoch: 44 [7000 (59%)]\tLoss: 0.000893\n",
      "Train Epoch: 45 [7100 (22%)]\tLoss: 0.000949\n",
      "Train Epoch: 45 [7200 (86%)]\tLoss: 0.001250\n",
      "Train Epoch: 46 [7300 (50%)]\tLoss: 0.001214\n",
      "Train Epoch: 47 [7400 (13%)]\tLoss: 0.001444\n",
      "Train Epoch: 47 [7500 (77%)]\tLoss: 0.001655\n",
      "Train Epoch: 48 [7600 (41%)]\tLoss: 0.002318\n",
      "Train Epoch: 49 [7700 (4%)]\tLoss: 0.001059\n",
      "Train Epoch: 49 [7800 (68%)]\tLoss: 0.001838\n",
      "test map: 0.423294126007022\n"
     ]
    }
   ],
   "source": [
    "args = ARGS(batch_size=32, test_batch_size=32, epochs=50, val_every=250, lr=1e-3, size=227, save_freq=10)\n",
    "model = ResNet\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=args.gamma)\n",
    "test_ap, test_map = trainer.train(args, model, optimizer, scheduler, model_name='runs/q3/model')\n",
    "print('test map:', test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imgs/q3_tb_map.png)\n",
    "![title](imgs/q3_tb_loss.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
